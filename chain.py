import os
import torch
import chainlit as cl
from chainlit.data.sql_alchemy import SQLAlchemyDataLayer
from transformers import AutoTokenizer, TextIteratorStreamer
from threading import Thread
import time
import json
import datetime

# å¼•å…¥è‡ªå®šä¹‰åº“
from utils import load_llama_or_qwen_model, replace_llama_with_packed_w2_layers

# ==============================================================================
# 0. å¹¶è¡Œä¸ç¯å¢ƒè®¾ç½® (å…³é”®ä¿®æ”¹åŒºåŸŸ)
# ==============================================================================

# ã€æ ¸å¿ƒè®¾ç½® 1ã€‘æŒ‡å®šä½ è¦ä½¿ç”¨çš„æ˜¾å¡ IDï¼ˆæ”¯æŒå‘½ä»¤è¡Œç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
# å‘½ä»¤è¡Œæœªè®¾ç½® TARGET_GPUS æ—¶ï¼Œé»˜è®¤ç”¨ "1"
TARGET_GPUS = os.getenv("TARGET_GPUS", "1")
os.environ["CUDA_VISIBLE_DEVICES"] = TARGET_GPUS

print(f">>> [System] å·²æŒ‡å®š GPU: {TARGET_GPUS}")


# ==============================================================================
# 1. å­˜å‚¨æœºåˆ¶é…ç½®
# ==============================================================================
current_dir = os.getcwd()
db_path = os.path.join(current_dir, "chat.db")
storage = SQLAlchemyDataLayer(conninfo=f"sqlite+aiosqlite:///{db_path}")
cl.data._data_layer = storage


def save_log_to_json(session_id, user_input, ai_output):
    """ä¿å­˜æ—¥å¿—åˆ° JSONL"""
    try:
        date_str = datetime.datetime.now().strftime("%Y-%m-%d")
        log_dir = os.path.join(current_dir, "chat_logs", date_str)
        os.makedirs(log_dir, exist_ok=True)
        file_path = os.path.join(log_dir, f"{session_id}.jsonl")
        record = {
            "timestamp": datetime.datetime.now().isoformat(),
            "role_user": user_input,
            "role_assistant": ai_output
        }
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    except Exception as e:
        print(f"!!! [Log] ä¿å­˜æ—¥å¿—å¤±è´¥: {e}")


# ==============================================================================
# 2. æ¨¡å‹åŠ è½½åŒº + é¢„çƒ­åŠŸèƒ½
# ==============================================================================
class Args:
    # æ”¯æŒç”¨ç¯å¢ƒå˜é‡è¦†ç›–é»˜è®¤è·¯å¾„
    quant_pt = os.getenv("QUANT_PT", "/data1/xjy/2-bit/true_quant.pth")
    model_path = os.getenv("MODEL_PATH", "/data2/llms/Qwen2.5-14B-Instruct")
    # group_size ä¹Ÿå¯ä»¥é¡ºä¾¿åšæˆå¯é…ç½®
    group_size = int(os.getenv("GROUP_SIZE", "64"))


# å…¨å±€å˜é‡
MODEL = None
TOKENIZER = None
WARMED_UP = False  # æ ‡è®°æ˜¯å¦å·²ç»é¢„çƒ­ï¼Œé¿å…é‡å¤é¢„çƒ­


def warmup_model(num_new_tokens: int = 16):
    """
    è¿›è¡Œä¸€æ¬¡æçŸ­çš„ dummy æ¨ç†ï¼Œç”¨äºé¢„çƒ­ CUDA kernel / KV cache ç­‰ï¼Œ
    é™ä½ç”¨æˆ·ç¬¬ä¸€è½®å¯¹è¯çš„å»¶è¿Ÿã€‚
    """
    global MODEL, TOKENIZER, WARMED_UP
    if MODEL is None or TOKENIZER is None:
        return

    if WARMED_UP:
        return

    print(">>> [System] æ­£åœ¨è¿›è¡Œæ¨¡å‹é¢„çƒ­ (warmup) ...")
    dummy_prompt = "Warmup."

    try:
        inputs = TOKENIZER(dummy_prompt, return_tensors="pt").to(MODEL.device)
        with torch.no_grad():
            _ = MODEL.generate(
                **inputs,
                max_new_tokens=num_new_tokens,
                do_sample=False,
                temperature=0.0,
                pad_token_id=TOKENIZER.eos_token_id,
                eos_token_id=TOKENIZER.eos_token_id,
            )

        # åŒæ­¥ä¸€ä¸‹ï¼Œç¡®ä¿æ‰€æœ‰ CUDA kernel å·²ç»æ‰§è¡Œå®Œæ¯•
        if torch.cuda.is_available():
            torch.cuda.synchronize()

        WARMED_UP = True
        print(">>> [System] é¢„çƒ­å®Œæˆï¼Œæ¨¡å‹å°±ç»ª âœ…")
    except Exception as e:
        # é¢„çƒ­å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œåªæ‰“å°ä¸€ä¸‹
        print(f"!!! [System] é¢„çƒ­è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸ï¼Œä½†ä¸ä¼šå½±å“æ­£å¸¸æ¨ç†: {e}")


def init_model():
    """åˆå§‹åŒ–æ¨¡å‹ï¼Œå¹¶åœ¨åŠ è½½å®Œæˆåè¿›è¡Œä¸€æ¬¡é¢„çƒ­"""
    global MODEL, TOKENIZER
    if MODEL is not None:
        return

    print(">>> [System] æ­£åœ¨åŠ è½½ Tokenizer...")
    TOKENIZER = AutoTokenizer.from_pretrained(Args.model_path, use_fast=True)

    print(">>> [System] æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ (CPU)...")
    # æ³¨æ„ï¼šè¿™é‡ŒåŠ è½½åˆ° CPU å†…å­˜ä¸­ï¼Œä¸è¦æ€¥ç€ .cuda()
    model = load_llama_or_qwen_model(Args.model_path)

    print(f">>> [System] æ­£åœ¨åŠ è½½é‡åŒ–æƒé‡: {Args.quant_pt}...")
    state_dict = torch.load(Args.quant_pt, map_location="cpu")  # ç¡®ä¿åŠ è½½åˆ° CPU

    print(">>> [System] æ­£åœ¨åº”ç”¨è‡ªå®šä¹‰ 2-bit é‡åŒ–å±‚ (CPU)...")
    # åœ¨ CPU ä¸Šå®Œæˆç»“æ„æ›¿æ¢ï¼Œé¿å…æ˜¾å­˜ç¢ç‰‡
    replace_llama_with_packed_w2_layers(model, state_dict, Args.group_size)

    print(">>> [System] æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU...")
    model.cuda()
    model.eval()

    MODEL = model
    print(">>> [System] æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")

    # ğŸ”¥ æ¨¡å‹åŠ è½½å®Œæ¯•åï¼Œç«‹åˆ»è¿›è¡Œä¸€æ¬¡è½»é‡é¢„çƒ­
    warmup_model()


# å¯åŠ¨æ—¶å°±å°è¯•åŠ è½½æ¨¡å‹ + é¢„çƒ­ï¼Œå¤±è´¥æ—¶ä¿æŒ UI å¯ç”¨ï¼ˆåªæ˜¯ä¸æ‰§è¡ŒçœŸå®æ¨ç†ï¼‰
try:
    init_model()
except Exception as e:
    print(f"!!! æ¨¡å‹åŠ è½½å¤±è´¥ (ä»… UI æ¨¡å¼ï¼Œå¯æŸ¥çœ‹æŠ¥é”™): {e}")


# ==============================================================================
# 3. Chainlit äº¤äº’é€»è¾‘
# ==============================================================================

@cl.on_chat_start
async def start():
    cl.user_session.set("history", [])
    if not cl.user_session.get("id"):
        cl.user_session.set("id", str(int(time.time())))

    await cl.Message(
        content=f"ğŸ‘‹ ä½ å¥½ï¼æˆ‘æ˜¯ NUDT Deep Mind ç ”å‘çš„å¯¹è¯åŠ©æ‰‹ (è¿è¡Œäº GPU {TARGET_GPUS})ï½"
    ).send()


@cl.on_chat_resume
async def on_resume(thread):
    steps = thread["steps"]
    history = []
    last_user_input = ""
    for step in steps:
        if step["type"] == "user_message":
            last_user_input = step["output"]
        elif step["type"] == "assistant_message":
            if last_user_input:
                history.append((last_user_input, step["output"]))
                last_user_input = ""
    cl.user_session.set("history", history)


@cl.on_message
async def main(message: cl.Message):
    user_input = message.content
    history = cl.user_session.get("history", [])

    # 1. æ„å»º Promptï¼ˆå¤šè½®å¯¹è¯æ‹¼æ¥ï¼‰
    messages = []
    for turn in history:
        messages.append({"role": "user", "content": turn[0]})
        messages.append({"role": "assistant", "content": turn[1]})
    messages.append({"role": "user", "content": user_input})

    try:
        full_prompt = TOKENIZER.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
    except Exception:
        # ä¸‡ä¸€æ¨¡æ¿ä¸å¯ç”¨ï¼Œå°±ç®€å•æ‹¼æ¥
        full_prompt = ""
        for m in messages:
            full_prompt += f"{m['role']}: {m['content']}\n"
        full_prompt += "assistant:"

    # 2. æ¨ç†
    msg = cl.Message(content="")
    await msg.send()
    response_text = ""

    if MODEL is None:
        # æ¨¡å‹æ²¡åŠ è½½æˆåŠŸæ—¶ï¼Œç»™ä¸€ä¸ªå ä½å›å¤ï¼Œé¿å…å‰ç«¯æŒ‚æ­»
        time.sleep(1)
        response_text = f"ã€æ¨¡æ‹Ÿã€‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯ï¼š{user_input}\nå½“å‰åå°æ¨¡å‹å°šæœªæˆåŠŸåŠ è½½ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚"
        await msg.stream_token(response_text)
    else:
        # ç¡®ä¿å·²ç»é¢„çƒ­è¿‡ï¼ˆç†è®ºä¸Šåœ¨ init_model ä¸­å·²ç»é¢„çƒ­ä¸€æ¬¡ï¼Œè¿™é‡Œåªæ˜¯å…œåº•ï¼‰
        warmup_model()

        # è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡
        inputs = TOKENIZER(full_prompt, return_tensors="pt").to(MODEL.device)

        streamer = TextIteratorStreamer(
            TOKENIZER,
            skip_prompt=True,
            skip_special_tokens=True
        )

        generation_kwargs = dict(
            input_ids=inputs.input_ids,
            streamer=streamer,
            max_new_tokens=2048,
            eos_token_id=TOKENIZER.eos_token_id,
            pad_token_id=TOKENIZER.eos_token_id,
            temperature=0.7,
            do_sample=True,
        )

        thread = Thread(target=MODEL.generate, kwargs=generation_kwargs)
        thread.start()

        for new_text in streamer:
            response_text += new_text
            await msg.stream_token(new_text)

        thread.join()

    history.append((user_input, response_text))
    cl.user_session.set("history", history)
    await msg.update()

    session_id = cl.user_session.get("id")
    save_log_to_json(session_id, user_input, response_text)
