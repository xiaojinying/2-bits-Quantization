
<div align="center">

# ğŸš€ 2-Bits-Quantization
### é¢å‘ç«¯ä¾§éƒ¨ç½²çš„å¤§æ¨¡å‹äºŒæ¯”ç‰¹å‹ç¼©æŠ€æœ¯åŠåº”ç”¨ (W2A16)

[![Python](https://img.shields.io/badge/Python-3.9-blue.svg?logo=python&logoColor=white)](https://www.python.org/)
[![CUDA](https://img.shields.io/badge/CUDA-12.4+-green.svg?logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-toolkit)
[![Platform](https://img.shields.io/badge/Platform-Linux-lightgrey.svg?logo=linux&logoColor=white)](https://www.openkylin.top/)
[![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](./LICENSE)

<br/>
<img src="./figs/fig1.png" alt="Project Architecture" width="100%" />
<br/>

<p align="center">
  <a href="#-ç®€ä»‹-introduction">ğŸ“– ç®€ä»‹</a> â€¢
  <a href="#-æ”¯æŒæ¨¡å‹-supported-models">ğŸ“Š æ”¯æŒæ¨¡å‹</a> â€¢
  <a href="#-å®‰è£…æŒ‡å—-installation">ğŸ› ï¸ ç¯å¢ƒå®‰è£…</a> â€¢
  <a href="#-å¿«é€Ÿå¼€å§‹-quick-start">âš¡ å¿«é€Ÿå¼€å§‹</a> â€¢
  <a href="#-å®éªŒç»“æœ-experiment-results">ğŸ“ˆ å®éªŒç»“æœ</a> â€¢
  <a href="#-webäº¤äº’-web-demo">ğŸŒ Web äº¤äº’</a>
</p>

</div>

---

## ğŸ“– ç®€ä»‹ Introduction

> **ğŸ’ æ‰“ç ´ä½æ¯”ç‰¹é‡åŒ–çš„ç²¾åº¦é­”å’’ï¼Œå®ç°çœŸæ­£çš„ W2A16 ç«¯ä¾§æ¨ç†ã€‚**

æœ¬é¡¹ç›®è‡´åŠ›äºè§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç«¯ä¾§è®¾å¤‡éƒ¨ç½²æ—¶çš„å­˜å‚¨ä¸è®¡ç®—ç“¶é¢ˆã€‚é€šè¿‡é‡‡ç”¨æä½æ¯”ç‰¹çš„ **2-bit Quantization** æŠ€æœ¯ï¼Œç»“åˆåˆ›æ–°çš„é‡åŒ–æ„ŸçŸ¥ç®—æ³•ä¸å¤šåŸŸä¼˜åŒ–ç­–ç•¥ï¼Œæˆ‘ä»¬æˆåŠŸåœ¨å¤§å¹…å‹ç¼©æ¨¡å‹ä½“ç§¯çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„æ¨ç†ç²¾åº¦ã€‚

ä¸ç°æœ‰çš„æ··åˆç²¾åº¦æ–¹æ¡ˆï¼ˆå¦‚ PB-LLM, BiLLMï¼‰ä¸åŒï¼Œæœ¬é¡¹ç›®å®ç°äº†**åŸç”Ÿçš„ 2-bit æƒé‡ä¸ 16-bit æ¿€æ´»ï¼ˆ$W2A16$ï¼‰**ã€‚é…åˆåŸºäº **CUTLASS** å®šåˆ¶çš„é«˜æ€§èƒ½ç®—å­ï¼Œä¸ä»…å®ç°äº†çœŸé‡åŒ–æ¨ç†ï¼ˆTrue Quantizationï¼‰ï¼Œæ›´åœ¨å®é™…éƒ¨ç½²ä¸­æ˜¾è‘—é™ä½äº†æ˜¾å­˜å ç”¨å¹¶æå‡äº†æ¨ç†é€Ÿåº¦ã€‚

### ğŸŒŸ æ ¸å¿ƒäº®ç‚¹ Key Features

* **ğŸ“¦ æè‡´å‹ç¼©**ï¼šé€šè¿‡ $W2A16$ é‡åŒ–ï¼Œå°†æ¨¡å‹ä½“ç§¯å‹ç¼©è‡³æé™ï¼Œé€‚åº”å—é™è¾¹ç¼˜è®¾å¤‡ã€‚
* **ğŸ¯ ç²¾åº¦ä¿æŒ**ï¼šé‡‡ç”¨å¤šåŸŸä¼˜åŒ–é‡åŒ–ç®—æ³•ï¼Œæœ‰æ•ˆç¼“è§£ä½æ¯”ç‰¹å¯¼è‡´çš„ç²¾åº¦å´©å¡Œé—®é¢˜ã€‚
* **ğŸš€ é«˜æ•ˆæ¨ç†**ï¼šå†…ç½®å®šåˆ¶åŒ– **CUDA Kernel**ï¼Œæ‹’ç»æ¨¡æ‹Ÿé‡åŒ–ï¼Œå®æ‰“å®çš„æ€§èƒ½æå‡ã€‚
* **ğŸ”Œ æ˜“äºé›†æˆ**ï¼šä¼˜é›…çš„ API è®¾è®¡ï¼Œæ”¯æŒé“¾å¼è°ƒç”¨ä¸æ’ä»¶æ‰©å±•ï¼Œå¯¹å¼€å‘è€…å‹å¥½ã€‚

---

## ğŸ“Š æ”¯æŒæ¨¡å‹ Supported Models

æˆ‘ä»¬ç›®å‰æ”¯æŒä»¥ä¸‹ä¸»æµå¼€æºå¤§æ¨¡å‹ç³»åˆ—ï¼š

| æ¨¡å‹ç³»åˆ— (Family) | å˜ä½“ (Variants) | çŠ¶æ€ (Status) | å¤‡æ³¨ |
| :--- | :--- | :---: | :--- |
| ğŸ¦™ **LLaMA-2** | 7B, 13B, 70B | âœ… Ready | ç»å…¸åŸºåº§ |
| ğŸ¦™ **LLaMA-3** | 8B, 70B | âœ… Ready | æœ€æ–°ä¸€ä»£ |
| ğŸ¤– **Qwen-2.5** | 7B, 14B, 32B... | âœ… Ready | ä¸­æ–‡èƒ½åŠ›å¼º |
| ğŸŒªï¸ **Mistral** | 7B, 8x7B | âœ… Ready | é«˜æ€§ä»·æ¯” |

---

## ğŸ› ï¸ å®‰è£…æŒ‡å— Installation

### 1. ç¯å¢ƒå‡†å¤‡ (Prerequisites)

è¯·ç¡®ä¿æ‚¨çš„ç‰©ç†ç¯å¢ƒæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
* **OS**: Linux (Ubuntu 20.04+ æ¨è)
* **Python**: 3.9
* **CUDA**: 12.4 (æ¨èä»¥è·å¾—æœ€ä½³æ€§èƒ½)
* **CMake**: 3.21+

### 2. å…‹éš†ä»“åº“ä¸ä¾èµ–å®‰è£…

```bash
# 1. å…‹éš†ä»“åº“
git clone [https://openatom.tech/openkylin/2-bits-Quantization.git](https://openatom.tech/openkylin/2-bits-Quantization.git)
cd 2-bits-Quantization

# 2. åˆ›å»ºå¹¶æ¿€æ´» Conda ç¯å¢ƒ
conda create -n Quant python=3.9 -y
conda activate Quant

# 3. å®‰è£… Python ä¾èµ–
pip install -r requirements.txt

# 4. åˆå§‹åŒ–ç¬¬ä¸‰æ–¹å­æ¨¡å— (âš ï¸ å¿…é¡»æ­¥éª¤)
git submodule update --init --recursive
````

### 3\. ç¼–è¯‘ CUDA ç®—å­ (Build Kernels)

æœ¬é¡¹ç›®åŒ…å«è‡ªå®šä¹‰ CUDA ç®—å­ï¼Œå¿…é¡»ç¼–è¯‘åæ‰èƒ½ä½¿ç”¨ã€‚

> [\!IMPORTANT]
> **ç¼–è¯‘å‰é…ç½®**ï¼š
> è¯·æ ¹æ®æ‚¨çš„ç³»ç»Ÿç¯å¢ƒï¼Œä¿®æ”¹ `build.sh` ä¸­çš„ç¯å¢ƒå˜é‡è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯ `DCMAKE_PREFIX_PATH` åŠ CUDA ç›¸å…³è·¯å¾„ã€‚

```bash
# æ‰§è¡Œç¼–è¯‘è„šæœ¬
bash build.sh
```

**æ•°æ®å‡†å¤‡**ï¼šè¯·å°†ä¸‹è½½çš„æ•°æ®é›†ï¼ˆWikiText-2, C4 ç­‰ï¼‰æ”¾ç½®äº `../dataset` ç›®å½•ï¼Œå¹¶å°†ç›®æ ‡æ¨¡å‹æƒé‡ä¸‹è½½è‡³æœ¬åœ°ã€‚

-----

## âš¡ å¿«é€Ÿå¼€å§‹ Quick Start

### 1\. å¯åŠ¨é‡åŒ– (Quantization)

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯¹æ¨¡å‹è¿›è¡Œ 2-bit é‡åŒ–ï¼š

```bash
# è®¾ç½® GPU
export CUDA_VISIBLE_DEVICES=0

# è¿è¡Œé‡åŒ–è„šæœ¬
python3 main.py \
    --model /path/to/YOUR-MODEL \
    --dataset wikitext2 \
    --true-sequential \
    --act-order \
    --wbits 2 \
    --group-size 64 \
    --nsamples 128 \
    --max-iter-num 4 \
    --iters-before-round 200 \
    --inner-iters-for-round 5 \
    --blockwise-minimize-epoch 4 \
    --round-fn gptq \
    --blockwise-minimize-lr 1.0e-5 \
    --train-LN \
    --save \
    --static-groups
```

### 2\. æ¨¡å‹æ¨ç† (Inference)

åŠ è½½é‡åŒ–åçš„æ¨¡å‹è¿›è¡Œæ¨ç†æµ‹è¯•ï¼š

```bash
export CUDA_VISIBLE_DEVICES=0

python3 main.py \
    --model /path/to/YOUR-MODEL \
    --inference \
    --quant_pt /path/to/your/quantized_model.pt \
    --group-size 64
```

-----

## ğŸ“ˆ å®éªŒç»“æœ Experiment Results

åœ¨W2A16æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨ä¸»æµæ•°æ®é›†ä¸Šå¯¹æ¯”äº† FP16ï¼ˆåŸºçº¿ï¼‰ã€Decouple-Q ã€OmniQuantä»¥åŠæœ¬æ–¹æ³•ï¼ˆOursï¼‰çš„æ€§èƒ½è¡¨ç°ã€‚
<br>
\<sub\>æ³¨ï¼šPPL ($\downarrow$) è¡¨ç¤ºå›°æƒ‘åº¦ï¼Œæ•°å€¼è¶Šä½è¶Šå¥½ï¼›Accuracy ($\uparrow$) è¡¨ç¤ºå‡†ç¡®ç‡ï¼Œæ•°å€¼è¶Šé«˜è¶Šå¥½ã€‚\</sub\>

<br/>
<img src="./figs/fig2.png" alt="Project Architecture" width="100%" />
<br/>

<br/>
<img src="./figs/fig3.png" alt="Project Architecture" width="100%" />
<br/>

æœ¬åœ°æ¨¡å‹æ¨ç†å»¶è¿Ÿå’Œæ˜¾å­˜å ç”¨ã€‚

<br/>
<img src="./figs/fig4.png" alt="Project Architecture" width="100%" />
<br/>

-----

## ğŸŒ Web äº¤äº’ Web Demo

æˆ‘ä»¬æä¾›äº†åŸºäº Chainlit çš„ Web ç•Œé¢ï¼Œæ–¹ä¾¿ç›´è§‚ä½“éªŒæ¨¡å‹æ•ˆæœã€‚

> [\!WARNING]
> **ç¯å¢ƒéš”ç¦»è­¦å‘Š**ï¼šç”±äº `datasets` åº“çš„ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼ŒWeb äº¤äº’åŠŸèƒ½**å¿…é¡»**åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Conda ç¯å¢ƒä¸­è¿è¡Œã€‚

### 1\. å‡†å¤‡ Web ç¯å¢ƒ

```bash
conda create -n Web python=3.9 -y
conda activate Web
pip install -r requirements_web.txt
```

### 2\. å¯åŠ¨æœåŠ¡

```bash
# æ›¿æ¢å‚æ•°å¹¶å¯åŠ¨
TARGET_GPUS="0" \
MODEL_PATH="/path/to/original/model" \
QUANT_PT="/path/to/quantized_model.pt" \
chainlit run chain.py -w --port 8000
```

å¯åŠ¨åï¼Œè¯·è®¿é—®ç»ˆç«¯è¾“å‡ºçš„ URL (é€šå¸¸ä¸º `http://localhost:8000`) è¿›è¡Œå¯¹è¯ã€‚

-----

## ğŸ’» å¼€å‘è°ƒç”¨ Developers Guide

é™¤äº† Web Demoï¼Œæˆ‘ä»¬è¿˜æä¾›äº†çµæ´»çš„ API æ¥å£å’Œæœ¬åœ° Python è°ƒç”¨æ–¹å¼ã€‚

### ğŸ“¡ æ–¹å¼ä¸€ï¼šéƒ¨ç½² API æœåŠ¡ (REST API)

åŸºäº `Uvicorn` å¯åŠ¨é«˜æ€§èƒ½ API æœåŠ¡ï¼š

```bash
# ç¡®ä¿åœ¨ Web ç¯å¢ƒä¸‹
conda activate Web

# è®¾ç½®ç¯å¢ƒå˜é‡å¹¶å¯åŠ¨æœåŠ¡
CUDA_VISIBLE_DEVICES=0 \
MODEL_PATH='/path/to/original/model' \
QUANT_PT='/path/to/quantized_model.pt' \
uvicorn llm_backend:app --host 0.0.0.0 --port 8000
```

å¯åŠ¨æˆåŠŸåï¼Œæ‚¨å¯ä»¥é€šè¿‡ POST è¯·æ±‚è®¿é—® `http://localhost:8000/api/chat` æ¥å£è¿›è¡Œå¯¹è¯ã€‚

### ğŸ æ–¹å¼äºŒï¼šæœ¬åœ° Python è°ƒç”¨ (Local Inference)

æ”¯æŒ DSPy é›†æˆï¼š

```python
import dspy
from llm_backend import LocalLlamaModule

# åˆå§‹åŒ–æ¨¡å‹
qa = LocalLlamaModule()

# è¿›è¡Œæ¨ç†
pred = qa(question="Who are you?")
```

-----







## ğŸ“„ è®¸å¯è¯ License

æœ¬é¡¹ç›®éµå¾ª [Apache 2.0](https://www.google.com/search?q=./LICENSE) è®¸å¯è¯ã€‚

-----
